Excelente iniciativa! Ao incluir ambos os modelos, o seu reposit√≥rio torna-se um laborat√≥rio completo de **Arquiteturas de Aten√ß√£o**, mostrando que voc√™ domina as duas principais varia√ß√µes do artigo original de 2017: o **Decoder-only** (GPT) e o **Encoder-Decoder** (NMT).

Aqui est√° a proposta de um `README.md` robusto e unificado para o seu reposit√≥rio:

---

# üß† Laborat√≥rio de LLMs: Arquiteturas Transformer

Este reposit√≥rio cont√©m implementa√ß√µes acad√©micas e visuais das duas principais varia√ß√µes da arquitetura **Transformer**. O objetivo √© demonstrar a aplica√ß√£o pr√°tica de mecanismos de aten√ß√£o em tarefas de modelagem de linguagem e tradu√ß√£o autom√°tica.

## üöÄ Modelos Implementados

### 1. GPT Acad√©mico (Decoder-only)

**Foco:** Modelagem de linguagem autoregressiva.
Este notebook implementa um modelo inspirado no GPT, onde o foco √© a predi√ß√£o do pr√≥ximo token baseado num contexto pr√©vio.

* **Mecanismo:** *Masked Self-Attention* (Causal).
* **Diferenciais:**
* Uso de tokens especiais `[SEP]` para delimita√ß√£o de contexto.
* C√°lculo de **Perplexity** para avalia√ß√£o de performance.
* Visualiza√ß√£o do **Espa√ßo Sem√¢ntico (PCA)**: Redu√ß√£o de embeddings 64D para 3D para an√°lise de agrupamentos de palavras.



### 2. Transformer NMT (Encoder-Decoder)

**Foco:** Tradu√ß√£o Autom√°tica Neuronal (Portugu√™s -> Ingl√™s).
Implementa√ß√£o da arquitetura cl√°ssica *Seq2Seq* para transformar sequ√™ncias de um dom√≠nio noutro.

* **Mecanismo:** *Cross-Attention* (o Decoder consulta o Encoder).
* **Diferenciais:**
* **Regulariza√ß√£o Ativa:** Implementa√ß√£o de Dropout (20%) e Data Augmentation para combater o *overfitting*.
* **Interface Interativa:** Fun√ß√£o de infer√™ncia palavra por palavra com suporte a input do utilizador.
* **Visualiza√ß√£o de Fibras 3D:** Mapeamento em tempo real das conex√µes de aten√ß√£o entre os dois idiomas.



---

## üî¨ Fundamentos Matem√°ticos

### Aten√ß√£o por Produto Escalar Escalonado

Ambos os modelos baseiam-se na f√≥rmula fundamental:


$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V$$

* No **GPT**, $M$ √© uma m√°scara triangular que impede o modelo de "olhar para o futuro".
* No **NMT**, a *Cross-Attention* utiliza $Q$ do Decoder e $K, V$ do Encoder para realizar o alinhamento sem√¢ntico.

### Regulariza√ß√£o e Generaliza√ß√£o

Para evitar que os modelos apenas "decorem" os datasets de treino, aplicamos:

* **Standardization Personalizada:** Limpeza rigorosa de strings via Regex no Grafo do TensorFlow.
* **Teacher Forcing:** Utilizado durante o treino para acelerar a converg√™ncia, fornecendo o alvo real como input para o passo seguinte.

---

## üìä Visualiza√ß√£o Avan√ßada

O diferencial deste reposit√≥rio √© a capacidade de "abrir a caixa preta" do modelo:

1. **Top-10 Probability Flow:** Gr√°ficos interativos (Plotly) que mostram as 10 palavras mais prov√°veis na sa√≠da da Softmax.
2. **Panor√¢mica 3D:** Visualiza√ß√£o ajustada (aspect ratio 3:1) para observar o fluxo de dados em sequ√™ncias longas sem cortes.

---

## üõ†Ô∏è Tecnologias Utilizadas

* **TensorFlow / Keras:** Constru√ß√£o das camadas customizadas e loops de treino.
* **Pandas / Numpy:** Manipula√ß√£o e aumento de dados sint√©ticos.
* **Plotly:** Visualiza√ß√£o din√¢mica e interativa das matrizes de aten√ß√£o.
* **Scikit-Learn:** Redu√ß√£o de dimensionalidade para an√°lise de Embeddings.

---

### Como utilizar este reposit√≥rio:

1. Comece pelo `GPT_Academico_Final_Corrigido` para entender a predi√ß√£o de texto.
2. Avance para o `Transformer NMT` para compreender como dois Transformers comunicam entre si atrav√©s da Cross-Attention.

---
