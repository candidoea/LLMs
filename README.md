## ğŸ§  Arquitetura Transformer Autoregressiva â€” DescriÃ§Ã£o TÃ©cnica

Este projeto implementa um **Transformer decoder-only autoregressivo** para modelagem de linguagem.
Dada uma sequÃªncia de tokens, o modelo aprende a estimar a distribuiÃ§Ã£o condicional do prÃ³ximo token:

<img width="168" height="18" alt="image" src="https://github.com/user-attachments/assets/144c058b-89fe-4495-931d-f5d42070d4d3" />


O treinamento maximiza a log-verossimilhanÃ§a dos dados (equivalente Ã  minimizaÃ§Ã£o da cross-entropy).

---

## ğŸ“¥ RepresentaÃ§Ã£o de Entrada

Cada token discreto (x_i) Ã© mapeado para um vetor contÃ­nuo por uma matriz de embeddings:

<img width="222" height="21" alt="image" src="https://github.com/user-attachments/assets/9de14097-9cc9-470a-9aa3-60083dd6a8de" />


Para incorporar ordem sequencial, adicionamos embeddings posicionais:

<img width="104" height="23" alt="image" src="https://github.com/user-attachments/assets/8e002768-5c87-4464-80f5-3f1556cb1c0d" />


onde (p_i) representa a posiÃ§Ã£o do token.

---

## ğŸ” Bloco Transformer

O modelo Ã© composto por (L) blocos idÃªnticos.
Cada bloco contÃ©m:

1. **Masked Multi-Head Self-Attention**
2. **Feed-Forward Network**
3. **ConexÃµes residuais**
4. **Layer Normalization**

Para a camada (l):

<img width="244" height="21" alt="image" src="https://github.com/user-attachments/assets/d7f15d45-664f-4e95-86a0-9700194c0d57" />


---

## ğŸ¯ Self-Attention com MÃ¡scara Causal

Para uma sequÃªncia de comprimento (T), definimos:

<img width="305" height="19" alt="image" src="https://github.com/user-attachments/assets/b9924958-6bce-438f-9377-17c757d28246" />


onde:

<img width="128" height="20" alt="image" src="https://github.com/user-attachments/assets/90669914-9d81-4b3a-a00c-4af66602f2ee" />

<img width="216" height="21" alt="image" src="https://github.com/user-attachments/assets/41919b05-6c74-4438-b27d-ca0b721f4048" />


A atenÃ§Ã£o escalada:

<img width="362" height="44" alt="image" src="https://github.com/user-attachments/assets/b9ddfb9f-f204-44a4-87a9-27c497ae1e98" />


A **mÃ¡scara causal** (M) impede acesso ao futuro:

<img width="178" height="65" alt="image" src="https://github.com/user-attachments/assets/8a093149-35d6-4e77-8e22-900e8025d5f7" />


Isso garante fatoraÃ§Ã£o autoregressiva da distribuiÃ§Ã£o conjunta.

---

## ğŸ§© Multi-Head Attention

A atenÃ§Ã£o Ã© aplicada em paralelo em (h) cabeÃ§as:

<img width="221" height="18" alt="image" src="https://github.com/user-attachments/assets/cdfd10c2-0d4d-4b83-9beb-c87f14db4c97" />


<img width="531" height="18" alt="image" src="https://github.com/user-attachments/assets/2eab5b78-2b0a-49af-a8e7-2ae870298e81" />


---

## ğŸ§® Feed-Forward Position-Wise

Aplicado independentemente em cada posiÃ§Ã£o:

<img width="275" height="18" alt="image" src="https://github.com/user-attachments/assets/8fa3af35-3b63-4dbf-94a2-a3d632755c0a" />


Normalmente:

<img width="94" height="18" alt="image" src="https://github.com/user-attachments/assets/892b1824-c2f9-4543-b5f7-b809fb2ee407" />


---

## ğŸ”„ Residual + Normalization

Cada subcamada usa residual connection:

<img width="349" height="19" alt="image" src="https://github.com/user-attachments/assets/eeb435ad-13bd-4387-aced-0613c9b99125" />


Isso melhora estabilidade do gradiente e convergÃªncia.

---

## ğŸ“¤ Camada de SaÃ­da

A representaÃ§Ã£o final:

<img width="65" height="17" alt="image" src="https://github.com/user-attachments/assets/18b3de0e-b8de-4a75-8dfc-0e1b886adb90" />


Ã© projetada no vocabulÃ¡rio:

<img width="213" height="20" alt="image" src="https://github.com/user-attachments/assets/80dc0ba6-72d9-4b82-9d70-5e6ebd3af4dc" />


Probabilidades:

<img width="242" height="18" alt="image" src="https://github.com/user-attachments/assets/c33e89a4-d0ae-4bb0-914b-fe64ede863e3" />


---

## ğŸ“ FunÃ§Ã£o de Perda

Treinamento por mÃ¡xima verossimilhanÃ§a:

<img width="243" height="51" alt="image" src="https://github.com/user-attachments/assets/c7806fdb-a83e-46e2-8cd6-e2a38ffce0e2" />


Equivalente Ã  **cross-entropy token-wise**.

---

## âœï¸ GeraÃ§Ã£o Autoregressiva

Durante inferÃªncia:

1. Entrada inicial (x_{1:t})
2. Computar (P(x_{t+1}))
3. Amostrar ou escolher argmax
4. Concatenar e repetir

---

## ğŸ“ Complexidade

Self-attention densa:

<img width="102" height="20" alt="image" src="https://github.com/user-attachments/assets/07713cf2-d48b-4093-bb53-44f823b65ef4" />


onde (T) Ã© o comprimento da sequÃªncia.

---

## âœ… Propriedades do Modelo

* Modelagem autoregressiva explÃ­cita
* DependÃªncias globais via atenÃ§Ã£o
* ParalelizaÃ§Ã£o total no treinamento
* Escalabilidade por profundidade e largura
* FatoraÃ§Ã£o causal garantida pela mÃ¡scara triangular

---
