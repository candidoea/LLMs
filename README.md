ğŸ§  Arquitetura Transformer Autoregressiva â€” DescriÃ§Ã£o TÃ©cnicaEste projeto implementa um Transformer decoder-only autoregressivo para modelagem de linguagem.Dada uma sequÃªncia de tokens, o modelo aprende a estimar a distribuiÃ§Ã£o condicional do prÃ³ximo token:$$P(x_t \mid x_1, x_2, \dots, x_{t-1})$$O treinamento maximiza a log-verossimilhanÃ§a dos dados (equivalente Ã  minimizaÃ§Ã£o da cross-entropy).ğŸ“¥ RepresentaÃ§Ã£o de EntradaCada token discreto ($x_i$) Ã© mapeado para um vetor contÃ­nuo por uma matriz de embeddings:$$e_i = E[x_i], \quad E \in \mathbb{R}^{|V| \times d_{model}}$$Para incorporar ordem sequencial, adicionamos embeddings posicionais:$$z_i^{(0)} = e_i + p_i$$onde ($p_i$) representa a posiÃ§Ã£o do token.ğŸ” Bloco TransformerO modelo Ã© composto por ($L$) blocos idÃªnticos. Cada bloco contÃ©m:Masked Multi-Head Self-AttentionFeed-Forward NetworkConexÃµes residuaisLayer NormalizationPara a camada ($l$):$$z^{(l)} = \text{TransformerBlock}(z^{(l-1)})$$ğŸ¯ Self-Attention com MÃ¡scara CausalPara uma sequÃªncia de comprimento ($T$), definimos:$$Q = XW_Q,\quad K = XW_K,\quad V = XW_V$$onde:$X \in \mathbb{R}^{T \times d_{model}}$$W_Q, W_K, W_V \in \mathbb{R}^{d_{model} \times d_k}$A atenÃ§Ã£o escalada:$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V$$A mÃ¡scara causal ($M$) impede acesso ao futuro:$$M_{ij} = \begin{cases} 0 & \text{se } j \le i \\ -\infty & \text{se } j > i \end{cases}$$Isso garante fatoraÃ§Ã£o autoregressiva da distribuiÃ§Ã£o conjunta.ğŸ§© Multi-Head AttentionA atenÃ§Ã£o Ã© aplicada em paralelo em ($h$) cabeÃ§as:$$\text{head}_i = \text{Attention}(Q_i, K_i, V_i)$$$$\text{MHA}(X) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W_O$$ğŸ§® Feed-Forward Position-WiseAplicado independentemente em cada posiÃ§Ã£o:$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$Normalmente: $d_{ff} \gg d_{model}$ğŸ”„ Residual + NormalizationCada subcamada usa residual connection:$$y = \text{LayerNorm}(x + \text{Sublayer}(x))$$ğŸ“¤ Camada de SaÃ­daA representaÃ§Ã£o final $H = z^{(L)}$ Ã© projetada no vocabulÃ¡rio:$$\text{logits} = HW_{out} + b$$Probabilidades:$$P(x_{t+1} \mid x_{\le t}) = \text{softmax}(\text{logits}_t)$$ğŸ“ FunÃ§Ã£o de PerdaTreinamento por mÃ¡xima verossimilhanÃ§a:$$\mathcal{L} = -\sum_{t=1}^{T} \log P(x_t \mid x_1, \dots, x_{t-1})$$Equivalente Ã  cross-entropy token-wise.âœï¸ GeraÃ§Ã£o AutoregressivaDurante inferÃªncia:Entrada inicial ($x_{1}$)Computar $P(x_{t+1})$Amostrar ou escolher argmaxConcatenar e repetirğŸ“ ComplexidadeSelf-attention densa:$$O(T^2 \cdot d_{model})$$onde ($T$) Ã© o comprimento da sequÃªncia.âœ… Propriedades do ModeloModelagem autoregressiva explÃ­citaDependÃªncias globais via atenÃ§Ã£oParalelizaÃ§Ã£o total no treinamentoFatoraÃ§Ã£o causal garantida pela mÃ¡scara triangular
