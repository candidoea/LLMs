## ğŸ§  Arquitetura Transformer Autoregressiva â€” DescriÃ§Ã£o TÃ©cnica

Este projeto implementa um **Transformer decoder-only autoregressivo** para modelagem de linguagem.
Dada uma sequÃªncia de tokens, o modelo aprende a estimar a distribuiÃ§Ã£o condicional do prÃ³ximo token:

$$P(x_t \mid x_1, x_2, \dots, x_{t-1})$$

O treinamento maximiza a log-verossimilhanÃ§a dos dados (equivalente Ã  minimizaÃ§Ã£o da cross-entropy).

---

## ğŸ“¥ RepresentaÃ§Ã£o de Entrada

Cada token discreto (x_1) Ã© mapeado para um vetor contÃ­nuo por uma matriz de embeddings:

$$e_i = E[x_i], \quad E \in \mathbb{R}^{|V| \times d_{model}}$$

Para incorporar ordem sequencial, adicionamos embeddings posicionais:

$$z_i^{(0)} = e_i + p_i$$

onde (p_i) representa a posiÃ§Ã£o do token.

---

## ğŸ” Bloco Transformer

O modelo Ã© composto por (L) blocos idÃªnticos. Cada bloco contÃ©m:

1. **Masked Multi-Head Self-Attention**
2. **Feed-Forward Network**
3. **ConexÃµes residuais**
4. **Layer Normalization**

Para a camada (l):

$$z^{(l)} = \text{TransformerBlock}(z^{(l-1)})$$

---

## ğŸ¯ Self-Attention com MÃ¡scara Causal

Para uma sequÃªncia de comprimento (T), definimos:

$$Q = XW_Q,\quad K = XW_K,\quad V = XW_V$$

onde:

* $X \in \mathbb{R}^{T \times d_{model}}$
* $W_Q, W_K, W_V \in \mathbb{R}^{d_{model} \times d_k}$

A atenÃ§Ã£o escalada:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V$$

A **mÃ¡scara causal** (M) impede acesso ao futuro:

$$M_{ij} = \begin{cases} 0 & \text{se } j \le i \\ -\infty & \text{se } j > i \end{cases}$$

---

## ğŸ§© Multi-Head Attention

A atenÃ§Ã£o Ã© aplicada em paralelo em (h) cabeÃ§as:

$$\text{head}_i = \text{Attention}(Q_i, K_i, V_i)$$

$$\text{MHA}(X) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W_O$$

---

## ğŸ§® Feed-Forward Position-Wise

Aplicado independentemente em cada posiÃ§Ã£o:

$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

---

## ğŸ”„ Residual + Normalization

Cada subcamada usa residual connection:

$$y = \text{LayerNorm}(x + \text{Sublayer}(x))$$

---

## ğŸ“¤ Camada de SaÃ­da

A representaÃ§Ã£o final $H = z^{(L)}$ Ã© projetada no vocabulÃ¡rio:

$$\text{logits} = HW_{out} + b$$

Probabilidades:

$$P(x_{t+1} \mid x_{\le t}) = \text{softmax}(\text{logits}_t)$$

---

## ğŸ“ FunÃ§Ã£o de Perda

Treinamento por mÃ¡xima verossimilhanÃ§a:

$$\mathcal{L} = -\sum_{t=1}^{T} \log P(x_t \mid x_1, \dots, x_{t-1})$$

---

## ğŸ“ Complexidade

Self-attention densa:

$$O(T^2 \cdot d_{model})$$

---

## âœ… Propriedades do Modelo

* Modelagem autoregressiva explÃ­cita
* DependÃªncias globais via atenÃ§Ã£o
* ParalelizaÃ§Ã£o total no treinamento
* FatoraÃ§Ã£o causal garantida pela mÃ¡scara triangular

---
